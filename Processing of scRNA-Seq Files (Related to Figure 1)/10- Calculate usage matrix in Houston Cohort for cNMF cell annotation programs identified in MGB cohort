#####################  Inside R ################################
library(dplyr)
library(Seurat)
options(bitmapType='cairo')
options(future.globals.maxSize = 8000 * 1024^2)

Matrix <- GetAssayData(object = Tumors.combined, slot = "counts")


########## This loads the genes that were used in the overall MGB cNMF (Top 4000 variable genes) ###########
Genes <- scan("./cnmf_run.overdispersed_genes.txt", what="")

Matrix2 <- Matrix[rownames(Matrix) %in% Genes,]

write.table(t(as.matrix(Matrix2)), file="./Houston_Raw_Counts_Variable_for_cNMF.txt", sep="\t", col.names=NA, quote=FALSE)

dim(Matrix2) ######### to find out the number for --numgenes below #################

q()

########################### Exit R #######################################

##### We run cNMF prepare script to normalize the raw matrix counts ###############

cnmf prepare --output-dir ./Calculate_Usage/ --name Calculate_Usage -c Houston_Raw_Counts_Variable_for_cNMF.txt -k 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 --n-iter 500 --total-workers 1 --seed 14 --numgenes 2990;


python;

################################## Inside Python #################################

import sklearn
import sklearn.decomposition
from sklearn.decomposition import non_negative_factorization
import numpy as np
import scanpy as sc
import csv
import scipy
import pandas as pd
 
########### Load the spectra consensus file from the MGB All cells cNMF run ###########
H = np.load("cnmf_run.spectra.k_18.dt_0_015.consensus.df.npz", allow_pickle=True)

H2 = pd.DataFrame(H['data'], columns = H['columns'], index = H['index']) 

########## Load the normalized raw counts matrix of the Houston cohort (Normalized by "prepare" script ######
X = sc.read_h5ad('/seq/epiprod02/Chadi/Glioblastoma/10X/Yun/Seurat_Object/Calculate_Usage/Calculate_Usage/cnmf_tmp/Calculate_Usage.norm_counts.h5ad')

X2 = X.X.toarray()

X3 = pd.DataFrame(data=X2, columns = X.var_names , index = X.obs.index)

H3 = H2.filter(items = X3.columns)

H4 = H3.to_numpy()

X5 = X2.astype(np.float64)

########## Perform the calculation ###########
test = sklearn.decomposition.non_negative_factorization(X5, W=None, H=H4, n_components= 18, init='random', update_H=False, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=1000, alpha=0.0, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, regularization=None, random_state=None, verbose=0, shuffle=False)

test2 = list(test)

pd.DataFrame(test2[0], columns=H['index'], index=X.obs.index).to_csv(path_or_buf="./Houston_Glioma_All_cells_Usage.txt", sep="\t", quoting=csv.QUOTE_NONE)


############ This script outputs a usage matrix which is then normalized per row to percentages (in each cell, the usages of the programs sums up to 100). This matrix is used for annotating cells ################
