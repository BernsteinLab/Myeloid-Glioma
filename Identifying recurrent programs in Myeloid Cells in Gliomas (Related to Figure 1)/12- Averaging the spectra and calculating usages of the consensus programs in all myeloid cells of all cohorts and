####### Programs that had high cosine similarity scores across the cohorts were combined together and there spectra scores were averaged to obtain a spectra for the consensus program #######
####### The spectra scores for each program can be found in cnmf outputs with the following name cnmf_run.spectra.k_*.dt_0_02.consensus.txt". These are the scores that get average across programs having high cosine similarity score #####

####### After we obtain the spectra for all consensus programs, we place the values in a data frame "consensus_spectra.txt" ######

####### Calucluate usages for MGB Cohort ##########

cnmf prepare --output-dir ./Calculate_Usage/ --name Calculate_Usage -c MGB_Myeloid_Matrix_Filtered_For_NMF_Round2.txt -k 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 --n-iter 500 --total-workers 1 --seed 25 --numgenes 2276;


python;

################################## Inside Python #################################

import sklearn
import sklearn.decomposition
from sklearn.decomposition import non_negative_factorization
import numpy as np
import scanpy as sc
import csv
import scipy
import pandas as pd
 

H = pd.read_csv('consensus_spectra.txt', sep='\t', index_col=0)

####### Upload the normalized merged matrix (output of cnmf prepare script) ##########

X = sc.read_h5ad('Calculate_Usage/Calculate_Usage/cnmf_tmp/Calculate_Usage.norm_counts.h5ad')

X2 = X.X.toarray()

X3 = pd.DataFrame(data=X2, columns = X.var_names , index = X.obs.index)

X3 = X3.filter(items = H.columns)

H4 = H3.to_numpy()

X5 = X3.astype(np.float64)

test = sklearn.decomposition.non_negative_factorization(X5, W=None, H=H4, n_components= 14, init='random', update_H=False, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=1000, alpha=0.0, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, regularization=None, random_state=None, verbose=0, shuffle=False)

test2 = list(test)

pd.DataFrame(test2[0], columns=H.index, index=X.obs.index).to_csv(path_or_buf="./MGB_Myeloid_cells_Usage.txt", sep="\t", quoting=csv.QUOTE_NONE)

############ The output "MGB_Myeloid_cells_Usage.txt" contain the usage values of all cells which can be then normalized to percentages (each row (cell) the values of the usages should be converted to percentages so that the sums of the program usages will be always 100 in every cell ##############



####### Calucluate usages for Houston Cohort ##########

cnmf prepare --output-dir ./Calculate_Usage/ --name Calculate_Usage -c Houston_Myeloid_Matrix_Filtered_For_NMF_Round2.txt -k 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 --n-iter 500 --total-workers 1 --seed 25 --numgenes 2276;


python;

################################## Inside Python #################################

import sklearn
import sklearn.decomposition
from sklearn.decomposition import non_negative_factorization
import numpy as np
import scanpy as sc
import csv
import scipy
import pandas as pd
 

H = pd.read_csv('consensus_spectra.txt', sep='\t', index_col=0)

####### Upload the normalized merged matrix (output of cnmf prepare script) ##########

X = sc.read_h5ad('Calculate_Usage/Calculate_Usage/cnmf_tmp/Calculate_Usage.norm_counts.h5ad')

X2 = X.X.toarray()

X3 = pd.DataFrame(data=X2, columns = X.var_names , index = X.obs.index)

X3 = X3.filter(items = H.columns)

H4 = H3.to_numpy()

X5 = X3.astype(np.float64)

test = sklearn.decomposition.non_negative_factorization(X5, W=None, H=H4, n_components= 14, init='random', update_H=False, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=1000, alpha=0.0, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, regularization=None, random_state=None, verbose=0, shuffle=False)

test2 = list(test)

pd.DataFrame(test2[0], columns=H.index, index=X.obs.index).to_csv(path_or_buf="./Houston_Myeloid_cells_Usage.txt", sep="\t", quoting=csv.QUOTE_NONE)

############ The output "Houston_Myeloid_cells_Usage.txt" contain the usage values of all cells which can be then normalized to percentages (each row (cell) the values of the usages should be converted to percentages so that the sums of the program usages will be always 100 in every cell ##############




####### Calucluate usages for Jackson's Cohort ##########

cnmf prepare --output-dir ./Calculate_Usage/ --name Calculate_Usage -c Jackson_Myeloid_Matrix_Filtered_For_NMF_Round2.txt -k 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 --n-iter 500 --total-workers 1 --seed 25 --numgenes 2276;


python;

################################## Inside Python #################################

import sklearn
import sklearn.decomposition
from sklearn.decomposition import non_negative_factorization
import numpy as np
import scanpy as sc
import csv
import scipy
import pandas as pd
 

H = pd.read_csv('consensus_spectra.txt', sep='\t', index_col=0)

####### Upload the normalized merged matrix (output of cnmf prepare script) ##########

X = sc.read_h5ad('Calculate_Usage/Calculate_Usage/cnmf_tmp/Calculate_Usage.norm_counts.h5ad')

X2 = X.X.toarray()

X3 = pd.DataFrame(data=X2, columns = X.var_names , index = X.obs.index)

X3 = X3.filter(items = H.columns)

H4 = H3.to_numpy()

X5 = X3.astype(np.float64)

test = sklearn.decomposition.non_negative_factorization(X5, W=None, H=H4, n_components= 14, init='random', update_H=False, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=1000, alpha=0.0, alpha_W=0.0, alpha_H='same', l1_ratio=0.0, regularization=None, random_state=None, verbose=0, shuffle=False)

test2 = list(test)

pd.DataFrame(test2[0], columns=H.index, index=X.obs.index).to_csv(path_or_buf="./Jackson_Myeloid_cells_Usage.txt", sep="\t", quoting=csv.QUOTE_NONE)

############ The output "Jackson_Myeloid_cells_Usage.txt" contain the usage values of all cells which can be then normalized to percentages (each row (cell) the values of the usages should be converted to percentages so that the sums of the program usages will be always 100 in every cell ##############
